{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Usecase: Calculating Cloud Coverage from a Sky-Cam Image | Domain: Climate Patterns","metadata":{}},{"cell_type":"markdown","source":"### Set up","metadata":{}},{"cell_type":"code","source":"!pip install git+https://github.com/openai/CLIP.git","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-09-13T10:52:43.127144Z","iopub.execute_input":"2023-09-13T10:52:43.127510Z","iopub.status.idle":"2023-09-13T10:53:02.746805Z","shell.execute_reply.started":"2023-09-13T10:52:43.127470Z","shell.execute_reply":"2023-09-13T10:53:02.745530Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !pip install git+https://github.com/openai/CLIP.git\n\nimport os, cv2, torch, clip, timm, pickle\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\n\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom catboost import CatBoostRegressor\nfrom xgboost import XGBRegressor\n\nfrom torch import nn\nfrom tqdm.autonotebook import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer","metadata":{"execution":{"iopub.status.busy":"2023-09-13T10:53:02.750450Z","iopub.execute_input":"2023-09-13T10:53:02.755527Z","iopub.status.idle":"2023-09-13T10:53:20.046184Z","shell.execute_reply.started":"2023-09-13T10:53:02.755485Z","shell.execute_reply":"2023-09-13T10:53:20.045170Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/sky-image-recent-dataset/cloud_data_cleaned1.csv\")\ndf = df[['image_name', 'opaque_clouds']]\ndf.columns = ['image', 'cloudcover']\nprint(\"Total Records: \", len(df))\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T10:53:20.047499Z","iopub.execute_input":"2023-09-13T10:53:20.048436Z","iopub.status.idle":"2023-09-13T10:53:20.577154Z","shell.execute_reply.started":"2023-09-13T10:53:20.048400Z","shell.execute_reply":"2023-09-13T10:53:20.576152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Test Split","metadata":{}},{"cell_type":"code","source":"x = df['image']\ny = df['cloudcover']\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.25, random_state = 48)\nx_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.30, random_state = 48)\n\nprint((x_train.shape, x_val.shape, x_test.shape), (y_train.shape, y_val.shape, y_test.shape))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T10:53:20.580118Z","iopub.execute_input":"2023-09-13T10:53:20.581050Z","iopub.status.idle":"2023-09-13T10:53:20.618362Z","shell.execute_reply.started":"2023-09-13T10:53:20.581012Z","shell.execute_reply":"2023-09-13T10:53:20.617476Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finetuned CLIP Model Loading","metadata":{}},{"cell_type":"code","source":"class CFG:\n    debug = False\n    #image_path = \"/kaggle/input/sky-image-dataset/data/data\"\n    captions_path = \".\"\n    batch_size = 64\n    num_workers = 4\n    head_lr = 1e-3\n    image_encoder_lr = 1e-4\n    text_encoder_lr = 1e-5\n    weight_decay = 1e-3\n    patience = 1\n    factor = 0.8\n    epochs = 12\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    # device = \"cpu\"\n\n    model_name = 'resnet50'\n    image_embedding = 2048\n    text_encoder_model = \"distilbert-base-uncased\"\n    text_embedding = 768\n    text_tokenizer = \"distilbert-base-uncased\"\n    max_length = 200\n\n    pretrained = True # for both image encoder and text encoder\n    trainable = True # for both image encoder and text encoder\n    temperature = 1.0\n\n    size = 224 \n\n    # For projection head: used for both image and text encoders\n    num_projection_layers = 1\n    projection_dim = 256 \n    dropout = 0.1","metadata":{"execution":{"iopub.status.busy":"2023-09-13T10:53:20.619674Z","iopub.execute_input":"2023-09-13T10:53:20.620054Z","iopub.status.idle":"2023-09-13T10:53:20.627668Z","shell.execute_reply.started":"2023-09-13T10:53:20.620028Z","shell.execute_reply":"2023-09-13T10:53:20.626785Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLIPModel(nn.Module):\n    def __init__(\n        self,\n        temperature=CFG.temperature,\n        image_embedding=CFG.image_embedding,\n        text_embedding=CFG.text_embedding,\n    ):\n        super().__init__()\n        self.image_encoder = ImageEncoder()\n        self.text_encoder = TextEncoder()\n        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n        self.temperature = temperature\n\n    def forward(self, batch):\n        # Getting Image and Text Features\n        image_features = self.image_encoder(batch[\"image\"])\n        text_features = self.text_encoder(\n            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n        )\n        # Getting Image and Text Embeddings (with same dimension)\n        image_embeddings = self.image_projection(image_features)\n        text_embeddings = self.text_projection(text_features)\n\n        # Calculating the Loss\n        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n        images_similarity = image_embeddings @ image_embeddings.T\n        texts_similarity = text_embeddings @ text_embeddings.T\n        targets = F.softmax(\n            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n        )\n        texts_loss = cross_entropy(logits, targets, reduction='none')\n        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n        return loss.mean()\n    \n    \n    \nclass ImageEncoder(nn.Module):\n    # Encode images to a fixed size vector\n    def __init__(self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained, num_classes=0, global_pool=\"avg\")\n        for p in self.model.parameters():\n            p.requires_grad = trainable\n\n    def forward(self, x):\n        return self.model(x)\n    \n    \n    \nclass TextEncoder(nn.Module):\n    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n        super().__init__()\n        if pretrained:\n            self.model = DistilBertModel.from_pretrained(model_name)\n        else:\n            self.model = DistilBertModel(config=DistilBertConfig())\n            \n        for p in self.model.parameters():\n            p.requires_grad = trainable\n\n        # W are using the CLS token hidden representation as the sentence's embedding\n        self.target_token_idx = 0\n\n    def forward(self, input_ids, attention_mask):\n        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        last_hidden_state = output.last_hidden_state\n        return last_hidden_state[:, self.target_token_idx, :]\n    \n\n    \nclass ProjectionHead(nn.Module):\n    def __init__(\n        self,\n        embedding_dim,\n        projection_dim=CFG.projection_dim,\n        dropout=CFG.dropout\n    ):\n        super().__init__()\n        self.projection = nn.Linear(embedding_dim, projection_dim)\n        self.gelu = nn.GELU()\n        self.fc = nn.Linear(projection_dim, projection_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(projection_dim)\n    \n    def forward(self, x):\n        projected = self.projection(x)\n        x = self.gelu(projected)\n        x = self.fc(x)\n        x = self.dropout(x)\n        x = x + projected\n        x = self.layer_norm(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-09-13T10:53:20.629403Z","iopub.execute_input":"2023-09-13T10:53:20.630175Z","iopub.status.idle":"2023-09-13T10:53:20.650742Z","shell.execute_reply.started":"2023-09-13T10:53:20.630143Z","shell.execute_reply":"2023-09-13T10:53:20.649754Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = CLIPModel().to(CFG.device)\nmodel.load_state_dict(torch.load(\"/kaggle/input/sky-image-recent-dataset/best.pt\", map_location = CFG.device))\nmodel.eval()","metadata":{"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-13T10:53:20.652307Z","iopub.execute_input":"2023-09-13T10:53:20.652797Z","iopub.status.idle":"2023-09-13T10:53:33.843788Z","shell.execute_reply.started":"2023-09-13T10:53:20.652766Z","shell.execute_reply":"2023-09-13T10:53:33.842866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Dataset Preparation For Regression Model\n","metadata":{}},{"cell_type":"code","source":"class SkyImage(Dataset):\n    def __init__(self, img_dir, labels): \n        self.img_dir = img_dir\n        self.img_labels = labels\n\n    def __len__(self):\n        return len(self.img_dir)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(\"/kaggle/input/sky-image-recent-dataset/Extracted Images/Extracted Images\", self.img_dir[idx])\n        #image = Image.open(img_path).convert(\"RGB\")\n        #image = preprocess(image)\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, (244, 244))\n        image = np.moveaxis(image, -1, 0)\n        \n        label = self.img_labels[idx]\n        return image, label","metadata":{"execution":{"iopub.status.busy":"2023-09-13T10:53:33.845309Z","iopub.execute_input":"2023-09-13T10:53:33.845681Z","iopub.status.idle":"2023-09-13T10:53:33.853695Z","shell.execute_reply.started":"2023-09-13T10:53:33.845647Z","shell.execute_reply":"2023-09-13T10:53:33.852825Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_images = SkyImage(x_train.to_list(), y_train.to_list())\nvalid_images = SkyImage(x_val.to_list(), y_val.to_list())\ntest_images = SkyImage(x_test.to_list(), y_test.to_list())","metadata":{"execution":{"iopub.status.busy":"2023-09-13T10:53:33.854693Z","iopub.execute_input":"2023-09-13T10:53:33.855056Z","iopub.status.idle":"2023-09-13T10:53:35.133704Z","shell.execute_reply.started":"2023-09-13T10:53:33.855024Z","shell.execute_reply":"2023-09-13T10:53:35.132478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building Features","metadata":{}},{"cell_type":"code","source":"def get_features(dataset):\n    \n    all_features = []\n    all_labels = []\n    all_embeddings = []\n    \n    with torch.no_grad():\n        for images, labels in tqdm(DataLoader(dataset, batch_size = 64)):\n            image_input = torch.tensor(np.stack(images)).cuda().float()\n            image_features = model.image_encoder(image_input)\n            image_embeddings = model.image_projection(image_features)\n            all_features.append(image_features)\n            all_labels.append(labels)\n            all_embeddings.append(image_embeddings)\n        \n    return torch.cat(all_features), torch.cat(all_labels).cuda(), torch.cat(all_embeddings).cuda()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T10:53:35.137042Z","iopub.execute_input":"2023-09-13T10:53:35.137568Z","iopub.status.idle":"2023-09-13T10:53:35.146250Z","shell.execute_reply.started":"2023-09-13T10:53:35.137527Z","shell.execute_reply":"2023-09-13T10:53:35.145161Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid_features, valid_labels, valid_embeddings = get_features(valid_images)\ntest_features, test_labels, test_embeddings = get_features(test_images)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T10:53:35.147935Z","iopub.execute_input":"2023-09-13T10:53:35.148301Z","iopub.status.idle":"2023-09-13T11:01:54.607777Z","shell.execute_reply.started":"2023-09-13T10:53:35.148269Z","shell.execute_reply":"2023-09-13T11:01:54.606774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_features, train_labels, train_embeddings = get_features(train_images)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T11:13:39.604049Z","iopub.execute_input":"2023-09-13T11:13:39.604420Z","iopub.status.idle":"2023-09-13T11:23:10.049299Z","shell.execute_reply.started":"2023-09-13T11:13:39.604390Z","shell.execute_reply":"2023-09-13T11:23:10.048263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Validation","metadata":{}},{"cell_type":"code","source":"print(len(train_features)==len(train_labels))\nprint(len(valid_features)==len(valid_labels))\nprint(len(test_features)==len(test_labels))\n\nprint(len(train_features), len(valid_features), len(test_features))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T11:23:15.619892Z","iopub.execute_input":"2023-09-13T11:23:15.620255Z","iopub.status.idle":"2023-09-13T11:23:15.626371Z","shell.execute_reply.started":"2023-09-13T11:23:15.620227Z","shell.execute_reply":"2023-09-13T11:23:15.625488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Evaluation Metrics","metadata":{}},{"cell_type":"code","source":"def evaluate(name, x, y, n, p): #p: features, #n: no of observations\n    print(\"---------------------------------------------------\")\n    print(\"{} MAE: {}\".format(name, mean_absolute_error(x, y)))\n    print(\"{} RMSE: {}\".format(name, mean_squared_error(x, y, squared = False)))\n    print(\"{} MSE: {}\".format(name, mean_squared_error(x, y)))\n    r2 = r2_score(x, y)\n    print(\"{} R2: {}\".format(name, r2))\n    #adr = 1 - ((1 - r2) * (n - 1)) / (n - p - 1)\n    #print(\"{} Adjusted R2: {}\".format(name, adr))\n    print(\"---------------------------------------------------\")","metadata":{"execution":{"iopub.status.busy":"2023-09-13T11:23:25.483668Z","iopub.execute_input":"2023-09-13T11:23:25.484701Z","iopub.status.idle":"2023-09-13T11:23:25.491848Z","shell.execute_reply.started":"2023-09-13T11:23:25.484667Z","shell.execute_reply":"2023-09-13T11:23:25.490776Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Catboost Model","metadata":{}},{"cell_type":"code","source":"CB_model = CatBoostRegressor(iterations = 700, learning_rate = 0.1, max_depth = 8, eval_metric = 'RMSE', random_seed = 48)\n\nCB_model.fit(train_features.cpu().numpy(), train_labels.cpu().numpy(), \n             eval_set = (valid_features.cpu().numpy(), valid_labels.cpu().numpy()), \n             use_best_model = True, plot = True, verbose = 50)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T11:24:15.415541Z","iopub.execute_input":"2023-09-13T11:24:15.415951Z","iopub.status.idle":"2023-09-13T11:37:58.654927Z","shell.execute_reply.started":"2023-09-13T11:24:15.415918Z","shell.execute_reply":"2023-09-13T11:37:58.653997Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cbt_train_pred = CB_model.predict(train_features.cpu().numpy())\ncbt_valid_pred = CB_model.predict(valid_features.cpu().numpy())\ncbt_test_pred = CB_model.predict(test_features.cpu().numpy())","metadata":{"execution":{"iopub.status.busy":"2023-09-13T11:38:22.444096Z","iopub.execute_input":"2023-09-13T11:38:22.444454Z","iopub.status.idle":"2023-09-13T11:38:35.673133Z","shell.execute_reply.started":"2023-09-13T11:38:22.444427Z","shell.execute_reply":"2023-09-13T11:38:35.672114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(min(train_labels.cpu()), max(train_labels.cpu()))\n# print(min(cbt_train_pred), max(cbt_train_pred))\n\n# print(min(valid_labels.cpu()), max(valid_labels.cpu()))\n# print(min(cbt_valid_pred), max(cbt_valid_pred))\n\n# print(min(test_labels.cpu()), max(test_labels.cpu()))\n# print(min(cbt_test_pred), max(cbt_test_pred))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T11:39:04.576877Z","iopub.execute_input":"2023-09-13T11:39:04.577579Z","iopub.status.idle":"2023-09-13T11:39:04.582190Z","shell.execute_reply.started":"2023-09-13T11:39:04.577545Z","shell.execute_reply":"2023-09-13T11:39:04.581122Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"evaluate(\"Train\", train_labels.cpu(), cbt_train_pred, len(cbt_train_pred), 1)\nevaluate(\"Valid\", valid_labels.cpu(), cbt_valid_pred, len(cbt_valid_pred), 1)\nevaluate(\"Test\", test_labels.cpu(), cbt_test_pred, len(cbt_test_pred), 1)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T11:39:12.660193Z","iopub.execute_input":"2023-09-13T11:39:12.660569Z","iopub.status.idle":"2023-09-13T11:39:12.682970Z","shell.execute_reply.started":"2023-09-13T11:39:12.660539Z","shell.execute_reply":"2023-09-13T11:39:12.681860Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Saving Model","metadata":{}},{"cell_type":"code","source":"pickle.dump(CB_model, open('cbr_featured_model.sav', 'wb'))","metadata":{"execution":{"iopub.status.busy":"2023-09-13T11:39:25.320523Z","iopub.execute_input":"2023-09-13T11:39:25.320910Z","iopub.status.idle":"2023-09-13T11:39:25.342227Z","shell.execute_reply.started":"2023-09-13T11:39:25.320877Z","shell.execute_reply":"2023-09-13T11:39:25.341302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Testing & Inference","metadata":{}},{"cell_type":"code","source":"class CFG:\n    debug = False\n    captions_path = \".\"\n    batch_size = 64\n    num_workers = 4\n    head_lr = 1e-3\n    image_encoder_lr = 1e-4\n    text_encoder_lr = 1e-5\n    weight_decay = 1e-3\n    patience = 1\n    factor = 0.8\n    epochs = 12\n    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    device = \"cpu\"\n    model_name = 'resnet50'\n    image_embedding = 2048\n    text_encoder_model = \"distilbert-base-uncased\"\n    text_embedding = 768\n    text_tokenizer = \"distilbert-base-uncased\"\n    max_length = 200\n    pretrained = True # for both image encoder and text encoder\n    trainable = True # for both image encoder and text encoder\n    temperature = 1.0\n    size = 224 \n    # For projection head: used for both image and text encoders\n    num_projection_layers = 1\n    projection_dim = 256 \n    dropout = 0.1","metadata":{"execution":{"iopub.status.busy":"2023-09-13T11:39:37.117010Z","iopub.execute_input":"2023-09-13T11:39:37.117366Z","iopub.status.idle":"2023-09-13T11:39:37.124031Z","shell.execute_reply.started":"2023-09-13T11:39:37.117336Z","shell.execute_reply":"2023-09-13T11:39:37.123061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class CLIPModel(nn.Module):\n    def __init__(\n        self,\n        temperature=CFG.temperature,\n        image_embedding=CFG.image_embedding,\n        text_embedding=CFG.text_embedding,\n    ):\n        super().__init__()\n        self.image_encoder = ImageEncoder()\n        self.text_encoder = TextEncoder()\n        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n        self.temperature = temperature\n\n    def forward(self, batch):\n        # Getting Image and Text Features\n        image_features = self.image_encoder(batch[\"image\"])\n        text_features = self.text_encoder(\n            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n        )\n        # Getting Image and Text Embeddings (with same dimension)\n        image_embeddings = self.image_projection(image_features)\n        text_embeddings = self.text_projection(text_features)\n\n        # Calculating the Loss\n        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n        images_similarity = image_embeddings @ image_embeddings.T\n        texts_similarity = text_embeddings @ text_embeddings.T\n        targets = F.softmax(\n            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n        )\n        texts_loss = cross_entropy(logits, targets, reduction='none')\n        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n        return loss.mean()","metadata":{"execution":{"iopub.status.busy":"2023-09-13T11:39:40.080778Z","iopub.execute_input":"2023-09-13T11:39:40.081172Z","iopub.status.idle":"2023-09-13T11:39:40.091866Z","shell.execute_reply.started":"2023-09-13T11:39:40.081142Z","shell.execute_reply":"2023-09-13T11:39:40.090789Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ImageEncoder(nn.Module):\n    # Encode images to a fixed size vector\n    def __init__(self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained, num_classes=0, global_pool=\"avg\")\n        for p in self.model.parameters():\n            p.requires_grad = trainable\n\n    def forward(self, x):\n        return self.model(x)\n    \n    \n    \nclass TextEncoder(nn.Module):\n    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n        super().__init__()\n        if pretrained:\n            self.model = DistilBertModel.from_pretrained(model_name)\n        else:\n            self.model = DistilBertModel(config=DistilBertConfig())\n            \n        for p in self.model.parameters():\n            p.requires_grad = trainable\n\n        # W are using the CLS token hidden representation as the sentence's embedding\n        self.target_token_idx = 0\n\n    def forward(self, input_ids, attention_mask):\n        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        last_hidden_state = output.last_hidden_state\n        return last_hidden_state[:, self.target_token_idx, :]\n    \n\n    \nclass ProjectionHead(nn.Module):\n    def __init__(\n        self,\n        embedding_dim,\n        projection_dim=CFG.projection_dim,\n        dropout=CFG.dropout\n    ):\n        super().__init__()\n        self.projection = nn.Linear(embedding_dim, projection_dim)\n        self.gelu = nn.GELU()\n        self.fc = nn.Linear(projection_dim, projection_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(projection_dim)\n    \n    def forward(self, x):\n        projected = self.projection(x)\n        x = self.gelu(projected)\n        x = self.fc(x)\n        x = self.dropout(x)\n        x = x + projected\n        x = self.layer_norm(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-09-13T11:39:40.951565Z","iopub.execute_input":"2023-09-13T11:39:40.952268Z","iopub.status.idle":"2023-09-13T11:39:40.966373Z","shell.execute_reply.started":"2023-09-13T11:39:40.952232Z","shell.execute_reply":"2023-09-13T11:39:40.965157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Custom DataLoader\nclass SkyImage(Dataset):\n    def __init__(self, img, label): \n        self.img = img\n        self.img_label = label\n    def __len__(self):\n        return len(self.img)\n    def __getitem__(self, idx):\n#         image = cv2.cvtColor(self.img[idx], cv2.COLOR_BGR2RGB)\n#         image = cv2.resize(image, (244, 244))\n#         image = np.moveaxis(image, -1, 0)\n#         label = self.img_label[idx]\n#         return image, label\n    \n        img_path = os.path.join(\"/kaggle/input/sky-image-recent-dataset/Extracted Images/Extracted Images\", self.img[idx])\n        image = cv2.imread(img_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = cv2.resize(image, (244, 244))\n        image = np.moveaxis(image, -1, 0)\n        label = self.img_label[idx]\n        return image, label\n\n\n# Generate Features using Custom Pretrained Clip\ndef get_features(clip_model, dataset):\n    features, label, embeddings = [], [], []\n    with torch.no_grad():\n        for images, labels in tqdm(DataLoader(dataset, batch_size = 64)):\n            image_input = torch.tensor(np.stack(images)).cpu().float()\n            image_features = clip_model.image_encoder(image_input)\n            image_embeddings = clip_model.image_projection(image_features)\n            features.append(image_features)\n            label.append(labels)\n            embeddings.append(image_embeddings)\n    return torch.cat(features), torch.cat(label).cpu(), torch.cat(embeddings)","metadata":{"execution":{"iopub.status.busy":"2023-09-13T11:39:48.277068Z","iopub.execute_input":"2023-09-13T11:39:48.277421Z","iopub.status.idle":"2023-09-13T11:39:48.287844Z","shell.execute_reply.started":"2023-09-13T11:39:48.277391Z","shell.execute_reply":"2023-09-13T11:39:48.286729Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Loading Clip Model & XgBoost Model\n\nCTBR_model = pickle.load(open(\"/kaggle/working/cbr_featured_model.sav\", 'rb'))\nclip_model = CLIPModel().to(CFG.device)\nclip_model.load_state_dict(torch.load(\"/kaggle/input/sky-image-recent-dataset/best.pt\", map_location = CFG.device))\nclip_model.eval()","metadata":{"scrolled":true,"_kg_hide-output":true,"execution":{"iopub.status.busy":"2023-09-13T11:40:03.500580Z","iopub.execute_input":"2023-09-13T11:40:03.500995Z","iopub.status.idle":"2023-09-13T11:40:09.580281Z","shell.execute_reply.started":"2023-09-13T11:40:03.500942Z","shell.execute_reply":"2023-09-13T11:40:09.579367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Validation Dataset Predictions\n\nfor i in range(250):\n    img_path = [\"/kaggle/input/sky-image-recent-dataset/Extracted Images/Extracted Images/\" + x_val.to_list()[i]]\n    lbl = [y_val.to_list()[i]] \n    \n    valid_image = SkyImage(img_path, lbl)\n    valid_features, valid_labels, valid_embeddings = get_features(clip_model, valid_image)\n\n    # Prediction on a sample image\n    yp_cbt = CTBR_model.predict(valid_features.cpu().numpy())\n    if yp_cbt < 0.0:\n        yp_cbt = 0.0\n    if yp_cbt > 100.0:\n        yp_cbt = 100.0\n\n    # Prediction vs Actual\n    print(\"CatBoost: Actual Cloud Coverage: {} | Predicted Cloud Coverage:{}\".format(round(lbl[0], 2), round(yp_cbt[0], 2)))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-09-13T11:41:00.057393Z","iopub.execute_input":"2023-09-13T11:41:00.057765Z","iopub.status.idle":"2023-09-13T11:41:50.675406Z","shell.execute_reply.started":"2023-09-13T11:41:00.057734Z","shell.execute_reply":"2023-09-13T11:41:50.674300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test Dataset Predictions\n\nfor i in range(250):\n    # Reading Random Image\n    img_path = [\"/kaggle/input/sky-image-recent-dataset/Extracted Images/Extracted Images/\" + x_test.to_list()[i]]\n    lbl = [y_test.to_list()[i]] \n    # print(img_path, lbl) [img_path], [target]\n    \n    # Processing Image and generating features using Clip\n    test_image = SkyImage(img_path, lbl)\n    test_features, test_labels, test_embeddings = get_features(clip_model, test_image)\n\n    # Prediction on a sample image\n    yp_cbt = CTBR_model.predict(test_features.cpu().numpy())\n    if yp_cbt < 0.0:\n        yp_cbt = 0.0\n    if yp_cbt > 100.0:\n        yp_cbt = 100.0\n\n    # Prediction vs Actual\n    print(\"CatBoost: Actual Cloud Coverage: {} | Predicted Cloud Coverage:{}\".format(round(lbl[0], 2), round(yp_cbt[0], 2)))","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2023-09-13T11:42:09.467190Z","iopub.execute_input":"2023-09-13T11:42:09.467588Z","iopub.status.idle":"2023-09-13T11:43:01.223078Z","shell.execute_reply.started":"2023-09-13T11:42:09.467556Z","shell.execute_reply":"2023-09-13T11:43:01.222096Z"},"trusted":true},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}