{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### Usecase: Calculating Cloud Coverage from a Sky-Cam Image | Domain: Climate Patterns ","metadata":{}},{"cell_type":"markdown","source":"**About CLIP Model:**\n\n> Contrastive Language-Image Pre-training.\n\n> Learns the relationship between a text and the image it describes.\n\n> Used in Image - Text similarity usecases.\n\n> Intuition behind using it: To extract image embeddings during inference. (i.e. using Image Encoder of Clip)\n\n> CLIP model trained with this strategy is better than those SOTA models those are trained using ImageNet like models.","metadata":{}},{"cell_type":"markdown","source":"### Set up","metadata":{}},{"cell_type":"code","source":"# Installing Dependencies and Libraries\n\n# !pip install timm\nimport os, cv2, gc, itertools, torch, pickle, timm\nimport numpy as np\nimport pandas as pd\nfrom tqdm.autonotebook import tqdm\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom torch import nn\nimport torch.nn.functional as F\nfrom transformers import DistilBertModel, DistilBertConfig, DistilBertTokenizer","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:25:37.637986Z","iopub.execute_input":"2023-09-12T08:25:37.638899Z","iopub.status.idle":"2023-09-12T08:25:37.648662Z","shell.execute_reply.started":"2023-09-12T08:25:37.638859Z","shell.execute_reply":"2023-09-12T08:25:37.647685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Importing Processed Data","metadata":{}},{"cell_type":"code","source":"df = pd.read_csv(\"/kaggle/input/sky-image-recent-dataset/cloud_data_cleaned1.csv\")\ndf = df[['image_name', 'label']]\ndf.columns = ['image', 'caption']\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:25:57.056173Z","iopub.execute_input":"2023-09-12T08:25:57.056538Z","iopub.status.idle":"2023-09-12T08:25:57.548902Z","shell.execute_reply.started":"2023-09-12T08:25:57.056510Z","shell.execute_reply":"2023-09-12T08:25:57.547732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Total Records: \", len(df))","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:26:18.220119Z","iopub.execute_input":"2023-09-12T08:26:18.220489Z","iopub.status.idle":"2023-09-12T08:26:18.227388Z","shell.execute_reply.started":"2023-09-12T08:26:18.220459Z","shell.execute_reply":"2023-09-12T08:26:18.226463Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setting up Hyper Parameters in Configurations","metadata":{}},{"cell_type":"code","source":"# Setting up config\nclass CFG:\n    debug = False\n    image_path = \"/kaggle/input/sky-image-recent-dataset/Extracted Images/Extracted Images\"\n    captions_path = \".\"\n    batch_size = 128\n    num_workers = 4\n    head_lr = 1e-3\n    image_encoder_lr = 1e-4\n    text_encoder_lr = 1e-5\n    weight_decay = 1e-3\n    patience = 3\n    factor = 0.8\n    epochs = 15\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    model_name = 'resnet50'\n    image_embedding = 2048\n    text_encoder_model = \"distilbert-base-uncased\"\n    text_embedding = 768\n    text_tokenizer = \"distilbert-base-uncased\"\n    max_length = 200\n\n    pretrained = True # for both image encoder and text encoder\n    trainable = True # for both image encoder and text encoder\n    temperature = 1.0\n\n    size = 224 \n\n    # For projection head: used for both image and text encoders\n    num_projection_layers = 1\n    projection_dim = 256 \n    dropout = 0.1","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:26:52.696383Z","iopub.execute_input":"2023-09-12T08:26:52.697068Z","iopub.status.idle":"2023-09-12T08:26:52.730545Z","shell.execute_reply.started":"2023-09-12T08:26:52.697032Z","shell.execute_reply":"2023-09-12T08:26:52.728627Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Setting up Utils","metadata":{}},{"cell_type":"code","source":"# Loss Meter\nclass AvgMeter:\n    def __init__(self, name=\"Metric\"):\n        self.name = name\n        self.reset()\n\n    def reset(self):\n        self.avg, self.sum, self.count = [0] * 3\n\n    def update(self, val, count=1):\n        self.count += count\n        self.sum += val * count\n        self.avg = self.sum / self.count\n\n    def __repr__(self):\n        text = f\"{self.name}: {self.avg:.4f}\"\n        return text\n\ndef get_lr(optimizer):\n    for param_group in optimizer.param_groups:\n        return param_group[\"lr\"]","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:27:01.032505Z","iopub.execute_input":"2023-09-12T08:27:01.033072Z","iopub.status.idle":"2023-09-12T08:27:01.045819Z","shell.execute_reply.started":"2023-09-12T08:27:01.033031Z","shell.execute_reply":"2023-09-12T08:27:01.044732Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Building Custom Dataset","metadata":{}},{"cell_type":"markdown","source":"> We need to encode both images and their describing texts. So, the dataset needs to **return both images and texts**. \n\n> We will use **DistilBERT** model (which is smaller than BERT but performs nearly as well as BERT) from **HuggingFace** library as our text encoder. \n\n> We need to **tokenize** the sentences (captions) with DistilBERT tokenizer and then feed the token ids (input_ids) and the attention masks to DistilBERT. \n\n> Dataset needs to take care of the tokenization as well.","metadata":{}},{"cell_type":"markdown","source":"> In the **\\_\\_init\\_\\_** we receive a tokenizer object which is actually a HuggingFace tokinzer. \n\n> This tokenizer will be loaded when running the model. \n\n> We are padding and truncating the captions to a specified max_length. \n\n> In the **\\_\\_getitem\\_\\_** we will first load an encoded caption which is a dictionary with keys input_ids and attention_mask, make tensors out of its values and after that we will load the corresponding image, transform and augment it and then we make it a tensor and put it in the dictionary with \"image\" as the key. \n\n> Finally we put the raw text of the caption with the key \"caption\" in the dictionary only for visualization purposes.Â \n\n> We can use additional data augmentations if we want to improve the model's performance.","metadata":{}},{"cell_type":"code","source":"class CLIPDataset(torch.utils.data.Dataset):\n    def __init__(self, image_filenames, captions, tokenizer, transforms):\n        \"\"\"\n        image_filenames and captions must have the same length; so, if there are\n        multiple captions for each image, the image_filenames must have repetitive\n        file names.\n        \"\"\"\n        self.image_filenames = image_filenames\n        self.captions = list(captions)\n        self.encoded_captions = tokenizer(list(captions), padding=True, truncation=True, max_length=CFG.max_length)\n        self.transforms = transforms\n\n    def __getitem__(self, idx):\n        item = {\n            key: torch.tensor(values[idx])\n            for key, values in self.encoded_captions.items()\n        }\n\n        image = cv2.imread(f\"{CFG.image_path}/{self.image_filenames[idx]}\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        image = self.transforms(image=image)['image']\n        item['image'] = torch.tensor(image).permute(2, 0, 1).float()\n        item['caption'] = self.captions[idx]\n        return item\n\n    def __len__(self):\n        return len(self.captions)\n\n\ndef get_transforms(mode=\"train\"):\n    if mode == \"train\":\n        return A.Compose(\n            [\n                A.Resize(CFG.size, CFG.size, always_apply=True),\n                A.Normalize(max_pixel_value=255.0, always_apply=True),\n            ]\n        )\n    else:\n        return A.Compose(\n            [\n                A.Resize(CFG.size, CFG.size, always_apply=True),\n                A.Normalize(max_pixel_value=255.0, always_apply=True),\n            ]\n        )","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:27:09.196080Z","iopub.execute_input":"2023-09-12T08:27:09.196433Z","iopub.status.idle":"2023-09-12T08:27:09.208564Z","shell.execute_reply.started":"2023-09-12T08:27:09.196404Z","shell.execute_reply":"2023-09-12T08:27:09.207195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Image Encoder","metadata":{}},{"cell_type":"markdown","source":"> Using PyTorch Image Models library (timm) here which makes a lot of different image models available from ResNets to EfficientNets and many more. \n\n> Here we will use a ResNet50 as our image encoder. \n\n> You can easily use torchvision library to use ResNets if you don't want to install a new library.\n\n> The following code encodes each image to a fixed size vector with the size of the model's output channels (in case of ResNet50 the vector size will be **2048**). This is the output after the **nn_._aptiveAvgPool2d()_** layer.","metadata":{}},{"cell_type":"code","source":"class ImageEncoder(nn.Module):\n    # Encode images to a fixed size vector\n    def __init__(self, model_name=CFG.model_name, pretrained=CFG.pretrained, trainable=CFG.trainable):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained, num_classes=0, global_pool=\"avg\")\n        for p in self.model.parameters():\n            p.requires_grad = trainable\n\n    def forward(self, x):\n        return self.model(x)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:27:13.521516Z","iopub.execute_input":"2023-09-12T08:27:13.521911Z","iopub.status.idle":"2023-09-12T08:27:13.528993Z","shell.execute_reply.started":"2023-09-12T08:27:13.521880Z","shell.execute_reply":"2023-09-12T08:27:13.527774Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Text Encoder","metadata":{}},{"cell_type":"markdown","source":"> Used DistilBERT as the text encoder. Like its bigger brother BERT, two special tokens will be added to the actual input tokens: **CLS** and **SEP** which mark the start and end of a sentence. \n\n> To grab the whole representation of a sentence we use the final representations of the CLS token and we hope that this representation captures the overall meaning of the sentence (caption). \n\n> In the case of DistilBERT (and also BERT) the output hidden representation for each token is a vector with size **768**. So, the whole caption will be encoded in the CLS token representation whose size is 768.","metadata":{}},{"cell_type":"code","source":"class TextEncoder(nn.Module):\n    def __init__(self, model_name=CFG.text_encoder_model, pretrained=CFG.pretrained, trainable=CFG.trainable):\n        super().__init__()\n        if pretrained:\n            self.model = DistilBertModel.from_pretrained(model_name)\n        else:\n            self.model = DistilBertModel(config=DistilBertConfig())\n            \n        for p in self.model.parameters():\n            p.requires_grad = trainable\n\n        # W are using the CLS token hidden representation as the sentence's embedding\n        self.target_token_idx = 0\n\n    def forward(self, input_ids, attention_mask):\n        output = self.model(input_ids=input_ids, attention_mask=attention_mask)\n        last_hidden_state = output.last_hidden_state\n        return last_hidden_state[:, self.target_token_idx, :]","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:27:17.271662Z","iopub.execute_input":"2023-09-12T08:27:17.272550Z","iopub.status.idle":"2023-09-12T08:27:17.280404Z","shell.execute_reply.started":"2023-09-12T08:27:17.272503Z","shell.execute_reply":"2023-09-12T08:27:17.279394Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Projection Head","metadata":{}},{"cell_type":"markdown","source":"> Used [Keras code example implementation](https://keras.io/examples/nlp/nl_image_search/) of projection head to write the following in PyTorch.\n\n> Now that we have encoded both our images and texts into fixed size vectors (2048 for image and 768 for text) we need to bring (project) them into a **new world** (!) with **similar dimensions** for both images and texts in order to be able to compare them and push apart the non-relevant image and texts and pull together those that match. \n\n> So, the following code will bring the 2048 and 768 dimensional vectors into a 256 (projection_dim) dimensional world, where we can **compare** them.\n\n> **embedding_dim** is the size of the input vector (2048 for images and 768 for texts) and \"projection_dim\" is the the size of the output vector which will be 256 for our case.","metadata":{}},{"cell_type":"code","source":"class ProjectionHead(nn.Module):\n    def __init__(\n        self,\n        embedding_dim,\n        projection_dim=CFG.projection_dim,\n        dropout=CFG.dropout\n    ):\n        super().__init__()\n        self.projection = nn.Linear(embedding_dim, projection_dim)\n        self.gelu = nn.GELU()\n        self.fc = nn.Linear(projection_dim, projection_dim)\n        self.dropout = nn.Dropout(dropout)\n        self.layer_norm = nn.LayerNorm(projection_dim)\n    \n    def forward(self, x):\n        projected = self.projection(x)\n        x = self.gelu(projected)\n        x = self.fc(x)\n        x = self.dropout(x)\n        x = x + projected\n        x = self.layer_norm(x)\n        return x","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:27:20.072009Z","iopub.execute_input":"2023-09-12T08:27:20.072388Z","iopub.status.idle":"2023-09-12T08:27:20.080219Z","shell.execute_reply.started":"2023-09-12T08:27:20.072358Z","shell.execute_reply":"2023-09-12T08:27:20.079058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CLIP Model Define\n","metadata":{}},{"cell_type":"markdown","source":"> In the forward function, we first encode the images and texts separately into fixed size vectors (with different dimensionalities). \n\n> After that, using separate projection modules we project them to that shared world (space).\n\n> Here the encodings will become of similar shape (256 in our case). \n\n> After that we will compute the loss. ","metadata":{}},{"cell_type":"markdown","source":"> We have image_embeddings, a matrix with shape (batch_size, 256) and text_embeddings with shape (batch_size, 256). \n\n> It means we have two groups of vectors instead of two single vectors. \n\n> How do we measure how similar two groups of vectors (two matrices) are to each other? \n\n> Again, with dot product (@ operator in PyTorch does the dot product or matrix multiplication in this case). \n\n> To be able to multiply these two matrices together, we transpose the second one. \n\n> We get a matrix with shape (batch_size, batch_size) which we will call logits. (temperature is equal to 1.0 in our case, so, it does not make a difference.","metadata":{}},{"cell_type":"code","source":"class CLIPModel(nn.Module):\n    def __init__(\n        self,\n        temperature=CFG.temperature,\n        image_embedding=CFG.image_embedding,\n        text_embedding=CFG.text_embedding,\n    ):\n        super().__init__()\n        self.image_encoder = ImageEncoder()\n        self.text_encoder = TextEncoder()\n        self.image_projection = ProjectionHead(embedding_dim=image_embedding)\n        self.text_projection = ProjectionHead(embedding_dim=text_embedding)\n        self.temperature = temperature\n\n    def forward(self, batch):\n        # Getting Image and Text Features\n        image_features = self.image_encoder(batch[\"image\"])\n        text_features = self.text_encoder(\n            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n        )\n        # Getting Image and Text Embeddings (with same dimension)\n        image_embeddings = self.image_projection(image_features)\n        text_embeddings = self.text_projection(text_features)\n\n        # Calculating the Loss\n        logits = (text_embeddings @ image_embeddings.T) / self.temperature\n        images_similarity = image_embeddings @ image_embeddings.T\n        texts_similarity = text_embeddings @ text_embeddings.T\n        targets = F.softmax(\n            (images_similarity + texts_similarity) / 2 * self.temperature, dim=-1\n        )\n        texts_loss = cross_entropy(logits, targets, reduction='none')\n        images_loss = cross_entropy(logits.T, targets.T, reduction='none')\n        loss =  (images_loss + texts_loss) / 2.0 # shape: (batch_size)\n        return loss.mean()\n\n\ndef cross_entropy(preds, targets, reduction='none'):\n    log_softmax = nn.LogSoftmax(dim=-1)\n    loss = (-targets * log_softmax(preds)).sum(1)\n    if reduction == \"none\":\n        return loss\n    elif reduction == \"mean\":\n        return loss.mean()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:27:27.474118Z","iopub.execute_input":"2023-09-12T08:27:27.474628Z","iopub.status.idle":"2023-09-12T08:27:27.487944Z","shell.execute_reply.started":"2023-09-12T08:27:27.474588Z","shell.execute_reply":"2023-09-12T08:27:27.487027Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Train Functions","metadata":{}},{"cell_type":"markdown","source":"> Load train and valid dataloaders, our model and then train and evaluate our model on those.","metadata":{}},{"cell_type":"code","source":"def make_train_valid_dfs(df):\n    train_dataframe = df.iloc[:130000,:]\n    valid_dataframe = df.iloc[130000:,:]\n    return train_dataframe.reset_index(drop=True), valid_dataframe.reset_index(drop=True)\n\n\ndef build_loaders(dataframe, tokenizer, mode):\n    transforms = get_transforms(mode=mode)\n    dataset = CLIPDataset(\n        dataframe[\"image\"].values,\n        dataframe[\"caption\"].values,\n        tokenizer=tokenizer,\n        transforms=transforms,\n    )\n    dataloader = torch.utils.data.DataLoader(\n        dataset,\n        batch_size=CFG.batch_size,\n        num_workers=CFG.num_workers,\n        shuffle=True if mode == \"train\" else False,\n    )\n    return dataloader","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:28:03.165081Z","iopub.execute_input":"2023-09-12T08:28:03.165524Z","iopub.status.idle":"2023-09-12T08:28:03.176279Z","shell.execute_reply.started":"2023-09-12T08:28:03.165489Z","shell.execute_reply":"2023-09-12T08:28:03.175257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train_epoch(model, train_loader, optimizer, lr_scheduler, step):\n    loss_meter = AvgMeter()\n    tqdm_object = tqdm(train_loader, total=len(train_loader))\n    for batch in tqdm_object:\n        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n        loss = model(batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if step == \"batch\":\n            lr_scheduler.step()\n\n        count = batch[\"image\"].size(0)\n        loss_meter.update(loss.item(), count)\n\n        tqdm_object.set_postfix(train_loss=loss_meter.avg, lr=get_lr(optimizer))\n    return loss_meter\n\n\ndef valid_epoch(model, valid_loader):\n    loss_meter = AvgMeter()\n\n    tqdm_object = tqdm(valid_loader, total=len(valid_loader))\n    for batch in tqdm_object:\n        batch = {k: v.to(CFG.device) for k, v in batch.items() if k != \"caption\"}\n        loss = model(batch)\n\n        count = batch[\"image\"].size(0)\n        loss_meter.update(loss.item(), count)\n\n        tqdm_object.set_postfix(valid_loss=loss_meter.avg)\n    return loss_meter","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:28:06.926883Z","iopub.execute_input":"2023-09-12T08:28:06.927752Z","iopub.status.idle":"2023-09-12T08:28:06.939741Z","shell.execute_reply.started":"2023-09-12T08:28:06.927694Z","shell.execute_reply":"2023-09-12T08:28:06.938750Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Model Training","metadata":{}},{"cell_type":"code","source":"# Train Validation Split\ntrain_df, valid_df = make_train_valid_dfs(df)\nprint(len(train_df), len(valid_df))\n\n# Image and Text Encoding\ntokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\ntrain_loader = build_loaders(train_df, tokenizer, mode=\"train\")\nvalid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:28:43.577306Z","iopub.execute_input":"2023-09-12T08:28:43.577667Z","iopub.status.idle":"2023-09-12T08:30:25.877244Z","shell.execute_reply.started":"2023-09-12T08:28:43.577637Z","shell.execute_reply":"2023-09-12T08:30:25.876222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Pretrained Model Usage\nmodel = CLIPModel().to(CFG.device)\nparams = [\n{\"params\": model.image_encoder.parameters(), \"lr\": CFG.image_encoder_lr},\n{\"params\": model.text_encoder.parameters(), \"lr\": CFG.text_encoder_lr},\n{\"params\": itertools.chain(\n    model.image_projection.parameters(), model.text_projection.parameters()\n), \"lr\": CFG.head_lr, \"weight_decay\": CFG.weight_decay}\n]\noptimizer = torch.optim.AdamW(params, weight_decay=0.)\nlr_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\noptimizer, mode=\"min\", patience=CFG.patience, factor=CFG.factor\n)\nstep = \"epoch\"","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:30:32.429417Z","iopub.execute_input":"2023-09-12T08:30:32.430097Z","iopub.status.idle":"2023-09-12T08:30:44.043489Z","shell.execute_reply.started":"2023-09-12T08:30:32.430060Z","shell.execute_reply":"2023-09-12T08:30:44.042488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Model Training\nbest_loss = float('inf')\nfor epoch in range(CFG.epochs):\n    print(f\"Epoch: {epoch + 1}\")\n    model.train()\n    train_loss = train_epoch(model, train_loader, optimizer, lr_scheduler, step)\n    model.eval()\n    with torch.no_grad():\n        valid_loss = valid_epoch(model, valid_loader)\n\n    if valid_loss.avg < best_loss:\n        best_loss = valid_loss.avg\n        torch.save(model.state_dict(), \"best.pt\")\n        print(\"Saved Best Model!\")\n\n    lr_scheduler.step(valid_loss.avg)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T08:30:54.264340Z","iopub.execute_input":"2023-09-12T08:30:54.264692Z","iopub.status.idle":"2023-09-12T12:01:14.041143Z","shell.execute_reply.started":"2023-09-12T08:30:54.264662Z","shell.execute_reply":"2023-09-12T12:01:14.039927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Saving Model and Configurations\n","metadata":{}},{"cell_type":"code","source":"with open('/kaggle/working/clip_mdl.pkl', 'wb') as f:\n    pickle.dump(model, f)\n    \nwith open('/kaggle/working/clip_cfg.pkl','wb') as f:\n    pickle.dump(CFG, f)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:05:24.097736Z","iopub.execute_input":"2023-09-12T12:05:24.098264Z","iopub.status.idle":"2023-09-12T12:05:25.010136Z","shell.execute_reply.started":"2023-09-12T12:05:24.098225Z","shell.execute_reply":"2023-09-12T12:05:25.009117Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Inference: Getting Image Embeddings","metadata":{}},{"cell_type":"markdown","source":"> In this function, we are loading the model that we saved after training, feeding it images in validation set and returning the image_embeddings with shape (valid_set_size, 256) and the model itself.","metadata":{}},{"cell_type":"code","source":"def get_image_embeddings(valid_df, model_path):\n    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n    valid_loader = build_loaders(valid_df, tokenizer, mode=\"valid\")\n    \n    model = CLIPModel().to(CFG.device)\n    model.load_state_dict(torch.load(model_path, map_location=CFG.device))\n    model.eval()\n    \n    valid_image_embeddings = []\n    with torch.no_grad():\n        for batch in tqdm(valid_loader):\n            image_features = model.image_encoder(batch[\"image\"].to(CFG.device))\n            image_embeddings = model.image_projection(image_features)\n            valid_image_embeddings.append(image_embeddings)\n    return model, torch.cat(valid_image_embeddings)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:05:57.792440Z","iopub.execute_input":"2023-09-12T12:05:57.793031Z","iopub.status.idle":"2023-09-12T12:05:57.800726Z","shell.execute_reply.started":"2023-09-12T12:05:57.792998Z","shell.execute_reply":"2023-09-12T12:05:57.799637Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"_, valid_df = make_train_valid_dfs(df)\nmodel, image_embeddings = get_image_embeddings(valid_df, \"best.pt\")","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:06:04.699421Z","iopub.execute_input":"2023-09-12T12:06:04.700021Z","iopub.status.idle":"2023-09-12T12:06:26.496527Z","shell.execute_reply.started":"2023-09-12T12:06:04.699987Z","shell.execute_reply":"2023-09-12T12:06:26.495337Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Finding Matches\n","metadata":{}},{"cell_type":"markdown","source":"> This function does the task that we wished our model would be capable of: it gets the model, image_embeddings, and a text query. \n\n> It will display the most relevant images from the validation set!","metadata":{}},{"cell_type":"code","source":"def find_matches(model, image_embeddings, query, image_filenames, n=9):\n    tokenizer = DistilBertTokenizer.from_pretrained(CFG.text_tokenizer)\n    encoded_query = tokenizer([query])\n    batch = {\n        key: torch.tensor(values).to(CFG.device)\n        for key, values in encoded_query.items()\n    }\n    with torch.no_grad():\n        text_features = model.text_encoder(\n            input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"]\n        )\n        text_embeddings = model.text_projection(text_features)\n    \n    image_embeddings_n = F.normalize(image_embeddings, p=2, dim=-1)\n    text_embeddings_n = F.normalize(text_embeddings, p=2, dim=-1)\n    dot_similarity = text_embeddings_n @ image_embeddings_n.T\n    \n    values, indices = torch.topk(dot_similarity.squeeze(0), n * 5)\n    matches = [image_filenames[idx] for idx in indices[::5]]\n    \n    _, axes = plt.subplots(3, 3, figsize=(10, 10))\n    for match, ax in zip(matches, axes.flatten()):\n        image = cv2.imread(f\"{CFG.image_path}/{match}\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        ax.imshow(image)\n        ax.axis(\"off\")\n    \n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:08:05.965172Z","iopub.execute_input":"2023-09-12T12:08:05.965603Z","iopub.status.idle":"2023-09-12T12:08:05.979011Z","shell.execute_reply.started":"2023-09-12T12:08:05.965570Z","shell.execute_reply":"2023-09-12T12:08:05.977850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"find_matches(model, \n             image_embeddings,\n             query=\"Low cloud coverage\",\n             image_filenames=valid_df['image'].values,\n             n=9)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:08:18.380462Z","iopub.execute_input":"2023-09-12T12:08:18.381060Z","iopub.status.idle":"2023-09-12T12:08:19.391192Z","shell.execute_reply.started":"2023-09-12T12:08:18.381026Z","shell.execute_reply":"2023-09-12T12:08:19.390315Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"find_matches(model, \n             image_embeddings,\n             query=\"High Cloud Coverage\",\n             image_filenames=valid_df['image'].values,\n             n=9)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:08:26.161604Z","iopub.execute_input":"2023-09-12T12:08:26.162284Z","iopub.status.idle":"2023-09-12T12:08:27.201891Z","shell.execute_reply.started":"2023-09-12T12:08:26.162250Z","shell.execute_reply":"2023-09-12T12:08:27.201019Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"find_matches(model, \n             image_embeddings,\n             query=\"Moderate Cloud Coverage\",\n             image_filenames=valid_df['image'].values,\n             n=9)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:08:31.730471Z","iopub.execute_input":"2023-09-12T12:08:31.731233Z","iopub.status.idle":"2023-09-12T12:08:33.011912Z","shell.execute_reply.started":"2023-09-12T12:08:31.731185Z","shell.execute_reply":"2023-09-12T12:08:33.011040Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"find_matches(model, \n             image_embeddings,\n             query=\"No Cloud Coverage\",\n             image_filenames=valid_df['image'].values,\n             n=9)","metadata":{"execution":{"iopub.status.busy":"2023-09-12T12:09:46.467410Z","iopub.execute_input":"2023-09-12T12:09:46.467846Z","iopub.status.idle":"2023-09-12T12:09:47.495008Z","shell.execute_reply.started":"2023-09-12T12:09:46.467782Z","shell.execute_reply":"2023-09-12T12:09:47.494132Z"},"trusted":true},"execution_count":null,"outputs":[]}],"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}}